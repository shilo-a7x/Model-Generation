{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Linear(32, 32)\n",
    "        self.logvar = nn.Linear(32, 32)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar, dim):\n",
    "    MSE = nn.MSELoss(reduction=\"sum\")\n",
    "    reconstruction_loss = MSE(recon_x, x.view(-1, dim))\n",
    "    KL_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + KL_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == \"torch.storage\" and name == \"_load_from_bytes\":\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location=\"cpu\")\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "\n",
    "with open('2_layer_real_models.pickle', 'rb') as f:\n",
    "    real_models = CPU_Unpickler(f).load()\n",
    "\n",
    "def state_dict_to_vec(state_dict):\n",
    "    flat_params = []\n",
    "    for param in state_dict.values():\n",
    "        flat_params.append(param.view(-1))\n",
    "    return torch.cat(flat_params)\n",
    "\n",
    "tensor_list = [state_dict_to_vec(x) for x in real_models]\n",
    "\n",
    "data = torch.stack(tensor_list).to(device)\n",
    "dim = data[0].size(0)\n",
    "# Normalize the data\n",
    "data_min = data.min()\n",
    "data_max = data.max()\n",
    "data_normalized = (data - data_min) / (data_max - data_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8486\n",
      "Epoch: 1, Loss: 0.6802\n",
      "Epoch: 2, Loss: 0.6125\n",
      "Epoch: 3, Loss: 0.5768\n",
      "Epoch: 4, Loss: 0.5543\n",
      "Epoch: 5, Loss: 0.5380\n",
      "Epoch: 6, Loss: 0.5288\n",
      "Epoch: 7, Loss: 0.5269\n",
      "Epoch: 8, Loss: 0.5217\n",
      "Epoch: 9, Loss: 0.5200\n",
      "Epoch: 10, Loss: 0.5191\n",
      "Epoch: 11, Loss: 0.5198\n",
      "Epoch: 12, Loss: 0.5190\n",
      "Epoch: 13, Loss: 0.5153\n",
      "Epoch: 14, Loss: 0.5171\n",
      "Epoch: 15, Loss: 0.5163\n",
      "Epoch: 16, Loss: 0.5170\n",
      "Epoch: 17, Loss: 0.5156\n",
      "Epoch: 18, Loss: 0.5149\n",
      "Epoch: 19, Loss: 0.5138\n",
      "Epoch: 20, Loss: 0.5148\n",
      "Epoch: 21, Loss: 0.5142\n",
      "Epoch: 22, Loss: 0.5148\n",
      "Epoch: 23, Loss: 0.5128\n",
      "Epoch: 24, Loss: 0.5141\n",
      "Epoch: 25, Loss: 0.5120\n",
      "Epoch: 26, Loss: 0.5122\n",
      "Epoch: 27, Loss: 0.5127\n",
      "Epoch: 28, Loss: 0.5114\n",
      "Epoch: 29, Loss: 0.5110\n",
      "Epoch: 30, Loss: 0.5114\n",
      "Epoch: 31, Loss: 0.5106\n",
      "Epoch: 32, Loss: 0.5103\n",
      "Epoch: 33, Loss: 0.5128\n",
      "Epoch: 34, Loss: 0.5121\n",
      "Epoch: 35, Loss: 0.5095\n",
      "Epoch: 36, Loss: 0.5121\n",
      "Epoch: 37, Loss: 0.5106\n",
      "Epoch: 38, Loss: 0.5094\n",
      "Epoch: 39, Loss: 0.5104\n",
      "Epoch: 40, Loss: 0.5109\n",
      "Epoch: 41, Loss: 0.5112\n",
      "Epoch: 42, Loss: 0.5093\n",
      "Epoch: 43, Loss: 0.5103\n",
      "Epoch: 44, Loss: 0.5101\n",
      "Epoch: 45, Loss: 0.5106\n",
      "Epoch: 46, Loss: 0.5111\n",
      "Epoch: 47, Loss: 0.5087\n",
      "Epoch: 48, Loss: 0.5094\n",
      "Epoch: 49, Loss: 0.5088\n",
      "Epoch: 50, Loss: 0.5099\n",
      "Epoch: 51, Loss: 0.5094\n",
      "Epoch: 52, Loss: 0.5098\n",
      "Epoch: 53, Loss: 0.5095\n",
      "Epoch: 54, Loss: 0.5078\n",
      "Epoch: 55, Loss: 0.5085\n",
      "Epoch: 56, Loss: 0.5072\n",
      "Epoch: 57, Loss: 0.5082\n",
      "Epoch: 58, Loss: 0.5092\n",
      "Epoch: 59, Loss: 0.5098\n",
      "Epoch: 60, Loss: 0.5076\n",
      "Epoch: 61, Loss: 0.5078\n",
      "Epoch: 62, Loss: 0.5082\n",
      "Epoch: 63, Loss: 0.5063\n",
      "Epoch: 64, Loss: 0.5070\n",
      "Epoch: 65, Loss: 0.5078\n",
      "Epoch: 66, Loss: 0.5071\n",
      "Epoch: 67, Loss: 0.5075\n",
      "Epoch: 68, Loss: 0.5078\n",
      "Epoch: 69, Loss: 0.5069\n",
      "Epoch: 70, Loss: 0.5084\n",
      "Epoch: 71, Loss: 0.5063\n",
      "Epoch: 72, Loss: 0.5087\n",
      "Epoch: 73, Loss: 0.5065\n",
      "Epoch: 74, Loss: 0.5070\n",
      "Epoch: 75, Loss: 0.5068\n",
      "Epoch: 76, Loss: 0.5065\n",
      "Epoch: 77, Loss: 0.5062\n",
      "Epoch: 78, Loss: 0.5066\n",
      "Epoch: 79, Loss: 0.5076\n",
      "Epoch: 80, Loss: 0.5081\n",
      "Epoch: 81, Loss: 0.5058\n",
      "Epoch: 82, Loss: 0.5062\n",
      "Epoch: 83, Loss: 0.5054\n",
      "Epoch: 84, Loss: 0.5058\n",
      "Epoch: 85, Loss: 0.5071\n",
      "Epoch: 86, Loss: 0.5071\n",
      "Epoch: 87, Loss: 0.5066\n",
      "Epoch: 88, Loss: 0.5052\n",
      "Epoch: 89, Loss: 0.5067\n",
      "Epoch: 90, Loss: 0.5057\n",
      "Epoch: 91, Loss: 0.5058\n",
      "Epoch: 92, Loss: 0.5070\n",
      "Epoch: 93, Loss: 0.5054\n",
      "Epoch: 94, Loss: 0.5076\n",
      "Epoch: 95, Loss: 0.5064\n",
      "Epoch: 96, Loss: 0.5053\n",
      "Epoch: 97, Loss: 0.5054\n",
      "Epoch: 98, Loss: 0.5063\n",
      "Epoch: 99, Loss: 0.5058\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(dim).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for tensor in data_normalized:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(tensor.view(-1, dim))\n",
    "        loss = loss_function(recon_batch, tensor, mu, logvar, dim)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: {}, Loss: {:.4f}\".format(epoch, train_loss / len(data_normalized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.33)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    torch.tensor(X_train, device=device, dtype=torch.float32),\n",
    "    torch.tensor(X_test, device=device, dtype=torch.float32),\n",
    "    torch.tensor(y_train, device=device, dtype=torch.long),\n",
    "    torch.tensor(y_test, device=device, dtype=torch.long),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.38\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from model import Iris2LayerClassifier\n",
    "\n",
    "\n",
    "NUM_OF_MODELS = 500\n",
    "\n",
    "\n",
    "# Generate weight matrices from the VAE model\n",
    "generated_tensors = []\n",
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    for _ in range(NUM_OF_MODELS):\n",
    "        z = torch.randn(1, 32).to(device)  \n",
    "        g = vae.decoder(z)\n",
    "        generated_tensors.append(g.view(-1))\n",
    "\n",
    "\n",
    "def vec_to_state_dict(model, flat_params):\n",
    "    target_state_dict = model.state_dict()\n",
    "    current_idx = 0\n",
    "\n",
    "    new_state_dict = {}\n",
    "    for name, param in target_state_dict.items():\n",
    "        num_elements = param.numel()\n",
    "        new_param = flat_params[current_idx : current_idx + num_elements].view(\n",
    "            param.shape\n",
    "        )\n",
    "        new_state_dict[name] = new_param\n",
    "        current_idx += num_elements\n",
    "\n",
    "    return new_state_dict\n",
    "temp_model = Iris2LayerClassifier()\n",
    "generated_models = [vec_to_state_dict(temp_model, v) for v in generated_tensors]\n",
    "\n",
    "\n",
    "for state_dict in generated_models:\n",
    "    model = Iris2LayerClassifier().to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_pred = model(X_test)\n",
    "        _, labels = torch.max(y_pred, 1)\n",
    "        accuracy = accuracy_score(y_test.cpu().numpy(), labels.cpu().numpy())\n",
    "        print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2_layer_generated_models.pickle\", \"wb\") as f:\n",
    "    pickle.dump(generated_models, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
